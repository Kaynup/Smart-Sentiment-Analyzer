{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1220b781-21f6-4aaa-9140-9d5d1824a606",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, json, praw, time, csv, os\n",
    "from ratelimit import limits, sleep_and_retry\n",
    "from itertools import islice\n",
    "\n",
    "# =============================\n",
    "# 1. Constants and Credentials\n",
    "# =============================\n",
    "\n",
    "REDDIT_CLIENT_ID = \"7XS2VDcdCA5iXt1n90nc7w\"\n",
    "REDDIT_CLIENT_SECRET = 'lgAmuWHjL5j7IofUCJQtU4xBQfEicQ'\n",
    "REDDIT_USER_AGENT = 'review_scraper/0.1 by u/Advanced-Ad-5151'\n",
    "OPENROUTER_API_KEY = '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'\n",
    "\n",
    "REDDIT_RATE_LIMIT_CALLS = 60\n",
    "REDDIT_RATE_LIMIT_PERIOD = 60  # seconds\n",
    "\n",
    "BATCH_SIZE = 5\n",
    "OUTPUT_CSV = \"labeled_comments.csv\"\n",
    "\n",
    "\n",
    "POST_LIMIT = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c945d3c-6073-4079-9b5d-9f04b7d01508",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching...Subreddit: r/madlads\n",
      "Saving files in r-madlads...\n",
      "Fetched and saved 20594 comments from r/madlads\n",
      "\n",
      "Fetching...Subreddit: r/bidenbro\n",
      "Exception occured: received 403 HTTP response\n",
      "Skipped bidenbro...\n",
      "\n",
      "Fetching...Subreddit: r/memeeconomy\n",
      "Saving files in r-memeeconomy...\n",
      "Fetched and saved 912 comments from r/memeeconomy\n",
      "\n",
      "Fetching...Subreddit: r/rarepuppers\n",
      "Saving files in r-rarepuppers...\n",
      "Fetched and saved 11500 comments from r/rarepuppers\n",
      "\n",
      "Fetching...Subreddit: r/wholesomememes\n",
      "Saving files in r-wholesomememes...\n",
      "Fetched and saved 10881 comments from r/wholesomememes\n",
      "\n",
      "Fetching...Subreddit: r/dankchristianmemes\n",
      "Saving files in r-dankchristianmemes...\n",
      "Fetched and saved 6812 comments from r/dankchristianmemes\n",
      "\n",
      "Fetching...Subreddit: r/terriblefacebookmemes\n",
      "Saving files in r-terriblefacebookmemes...\n",
      "Fetched and saved 23860 comments from r/terriblefacebookmemes\n",
      "\n",
      "Fetching...Subreddit: r/dank_meme\n",
      "Saving files in r-dank_meme...\n",
      "Fetched and saved 3353 comments from r/dank_meme\n",
      "\n",
      "Fetching...Subreddit: r/trebuchetmemes\n",
      "Exception occured: received 403 HTTP response\n",
      "Skipped trebuchetmemes...\n",
      "\n",
      "Fetching...Subreddit: r/deepfriedmemes\n",
      "Saving files in r-deepfriedmemes...\n",
      "Fetched and saved 5827 comments from r/deepfriedmemes\n",
      "\n",
      "Fetching...Subreddit: r/delightfullychubby\n",
      "Saving files in r-delightfullychubby...\n",
      "Fetched and saved 2413 comments from r/delightfullychubby\n",
      "\n",
      "Fetching...Subreddit: r/Memes_Of_The_Dank\n",
      "Saving files in r-Memes_Of_The_Dank...\n",
      "Fetched and saved 5853 comments from r/Memes_Of_The_Dank\n",
      "\n",
      "Fetching...Subreddit: r/smoobypost\n",
      "Saving files in r-smoobypost...\n",
      "Fetched and saved 4023 comments from r/smoobypost\n",
      "\n",
      "Fetching...Subreddit: r/lotrmemes\n",
      "Saving files in r-lotrmemes...\n",
      "Fetched and saved 14920 comments from r/lotrmemes\n",
      "\n",
      "Fetching...Subreddit: r/ilikthebred\n",
      "Exception occured: received 403 HTTP response\n",
      "Skipped ilikthebred...\n",
      "\n",
      "Fetching...Subreddit: r/meme\n",
      "Saving files in r-meme...\n",
      "Fetched and saved 6167 comments from r/meme\n",
      "\n",
      "Fetching...Subreddit: r/garlicbreadmemes\n",
      "Saving files in r-garlicbreadmemes...\n",
      "Fetched and saved 4389 comments from r/garlicbreadmemes\n",
      "\n",
      "Fetching...Subreddit: r/offensivememes\n",
      "Exception occured: received 404 HTTP response\n",
      "Skipped offensivememes...\n",
      "\n",
      "Fetching...Subreddit: r/gocommitdie\n",
      "Saving files in r-gocommitdie...\n",
      "Fetched and saved 9233 comments from r/gocommitdie\n",
      "\n",
      "Fetching...Subreddit: r/wholesomegreentext\n",
      "Saving files in r-wholesomegreentext...\n",
      "Fetched and saved 9850 comments from r/wholesomegreentext\n",
      "\n",
      "Fetching...Subreddit: r/raimememes\n",
      "Exception occured: Redirect to /subreddits/search\n",
      "Skipped raimememes...\n",
      "\n",
      "Fetching...Subreddit: r/otmemes\n",
      "Saving files in r-otmemes...\n",
      "Fetched and saved 6645 comments from r/otmemes\n",
      "\n",
      "Fetching...Subreddit: r/kappa\n",
      "Saving files in r-kappa...\n",
      "Fetched and saved 10026 comments from r/kappa\n",
      "\n",
      "Fetching...Subreddit: r/equelmemes\n",
      "Saving files in r-equelmemes...\n",
      "Fetched and saved 6725 comments from r/equelmemes\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# =============================\n",
    "# 2. Set Up Reddit API Connection\n",
    "# =============================\n",
    "reddit = praw.Reddit(\n",
    "    client_id=REDDIT_CLIENT_ID,\n",
    "    client_secret=REDDIT_CLIENT_SECRET,\n",
    "    user_agent=REDDIT_USER_AGENT\n",
    ")\n",
    "\n",
    "# =============================\n",
    "# 3. Rate-Limited Reddit Call\n",
    "# =============================\n",
    "@sleep_and_retry\n",
    "@limits(calls=REDDIT_RATE_LIMIT_CALLS, period=REDDIT_RATE_LIMIT_PERIOD)\n",
    "def fetch_submission_comments(submission):\n",
    "    submission.comments.replace_more(limit=0)\n",
    "    return [comment.body for comment in submission.comments]\n",
    "\n",
    "# =============================\n",
    "# 4. Extract Comments from Subreddit\n",
    "# =============================\n",
    "def fetch_comments_from_subreddit(subreddit_name, post_limit=POST_LIMIT):\n",
    "    subreddit = reddit.subreddit(subreddit_name)\n",
    "    all_comments = []\n",
    "\n",
    "    for submission in subreddit.hot(limit=post_limit):\n",
    "        try:\n",
    "            comments = fetch_submission_comments(submission)\n",
    "            all_comments.extend(comments)\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching comments from {submission.title}: {e}\")\n",
    "\n",
    "    return all_comments\n",
    "\n",
    "# =============================\n",
    "# 5. Save Functions\n",
    "# =============================\n",
    "def save_comments_to_txt(comments, path):\n",
    "    with open(os.path.join(path, \"reddit_comments.txt\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        for comment in comments:\n",
    "            f.write(comment + \"\\n\")\n",
    "\n",
    "def save_comments_to_csv(comments, path):\n",
    "    with open(os.path.join(path, \"reddit_comments.csv\"), \"w\", encoding=\"utf-8\", newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([\"comment\"])\n",
    "        for comment in comments:\n",
    "            writer.writerow([comment])\n",
    "\n",
    "def save_comments_to_json(comments, path):\n",
    "    with open(os.path.join(path, \"reddit_comments.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(comments, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "# =============================\n",
    "# 6. Run Script\n",
    "# =============================\n",
    "subreddits = []\n",
    "\n",
    "for sub_name in subreddits:\n",
    "    print(f\"Fetching...Subreddit: r/{sub_name}\")\n",
    "    try:\n",
    "        comments = fetch_comments_from_subreddit(sub_name)\n",
    "    \n",
    "        folder_name = f\"r-{sub_name}\"\n",
    "        os.makedirs(folder_name, exist_ok=True)\n",
    "    \n",
    "        # Save to files inside folders, respectively\n",
    "        print(f\"Saving files in {folder_name}...\")\n",
    "        save_comments_to_txt(comments, folder_name)\n",
    "        save_comments_to_csv(comments, folder_name)\n",
    "        save_comments_to_json(comments, folder_name)\n",
    "    \n",
    "        print(f\"Fetched and saved {len(comments)} comments from r/{sub_name}\\n\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Exception occured: {e}\")\n",
    "        print(f\"Skipped {sub_name}...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67868327-1b63-4f72-8060-eefd0873f475",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e8eccd-804d-4c3a-b038-e783cb3e0271",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1278bf66-d610-4858-a7aa-5001422970c0",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "raw",
   "id": "72f101e1-2296-43fc-9591-468ae419f6b5",
   "metadata": {},
   "source": [
    "# =============================\n",
    "# 5. Chunk Helper\n",
    "# =============================\n",
    "\n",
    "def chunked_iterable(iterable, size):\n",
    "    it = iter(iterable)\n",
    "    return iter(lambda: list(islice(it, size)), [])\n",
    "\n",
    "# =============================\n",
    "# 6. Classification Request\n",
    "# =============================\n",
    "\n",
    "def classify_comments(comments_batch):\n",
    "    numbered_reviews = \"\\n\".join([f\"{i+1}. {r}\" for i, r in enumerate(comments_batch)])\n",
    "\n",
    "    classification_prompt = f\"\"\"\n",
    "Classify each review using the following features:\n",
    "\n",
    "Output in CSV format with these columns:\n",
    "\"text\",\"label\",\"tone\",\"context\",\"topic\",\"contains_slang\",\"complexity\",\"causal\"\n",
    "\n",
    "Note:- Do not add filler and narrative words from your side, only give csv format with commas directly start with \"text\", \"label\"..... and so on\n",
    "\n",
    "Columns:\n",
    "- text: the original review\n",
    "- label: negative, neutral, positive\n",
    "- tone: sarcastic, sincere, or neutral\n",
    "- context: review, comment\n",
    "- topic: e.g., tech, food, relationship, work, daily life, politics, weather, entertainment, etc\n",
    "- contains_slang: true or false\n",
    "- complexity: low or medium\n",
    "- causal: true or false\n",
    "\n",
    "Try cleaning the data also, do not remove normal punctuations like - comma, full stops, inverted commas, exclaimation or question marks etc.\n",
    "Remove emojies, links, brackets and other noisy sentences, remove deleted sentences too\n",
    "\n",
    "Reviews:\n",
    "{numbered_reviews}\n",
    "\"\"\".strip()\n",
    "\n",
    "    response = requests.post(\n",
    "        url=\"https://openrouter.ai/api/v1/chat/completions\",\n",
    "        headers={\n",
    "            \"Authorization\": f\"Bearer {OPENROUTER_API_KEY}\",\n",
    "            \"Content-Type\": \"application/json\"\n",
    "        },\n",
    "        data=json.dumps({\n",
    "            \"model\": \"deepseek/deepseek-r1-0528:free\",\n",
    "            \"messages\": [{\"role\": \"user\", \"content\": classification_prompt}]\n",
    "        })\n",
    "    )\n",
    "\n",
    "    return response\n",
    "\n",
    "# =============================\n",
    "# 7. Parse Response and Display or Save\n",
    "# =============================\n",
    "\n",
    "def parse_and_append(response, output_path):\n",
    "    try:\n",
    "        result = response.json()\n",
    "        message_text = result['choices'][0]['message']['content']\n",
    "\n",
    "        if not message_text.startswith('\"text\",\"label\"'):\n",
    "            print(\"Unexpected format:\\n\", message_text)\n",
    "            return\n",
    "\n",
    "        rows = list(csv.reader(message_text.strip().split('\\n')))\n",
    "        headers, *data = rows\n",
    "\n",
    "        print(f\"\\nProcessed {len(data)} comments:\")\n",
    "        for row in data:\n",
    "            print(\" | \".join(row))\n",
    "\n",
    "        ##Uncomment below to save to CSV\n",
    "        with open(output_path, 'a', newline='', encoding='utf-8') as f:\n",
    "            writer = csv.writer(f)\n",
    "            if f.tell() == 0:\n",
    "                writer.writerow(headers)\n",
    "            writer.writerows(data)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"Error parsing response:\", e)\n",
    "        print(\"Raw response:\", json.dumps(response.json(), indent=2))\n",
    "\n",
    "# =============================\n",
    "# 8. Run Classification Pipeline\n",
    "# =============================\n",
    "\n",
    "def process_comments(comments):\n",
    "    for i, comment_batch in enumerate(chunked_iterable(comments, BATCH_SIZE), 1):\n",
    "        print(f\"\\n=== Batch {i} ===\")\n",
    "        response = classify_comments(comment_batch)\n",
    "        parse_and_append(response, OUTPUT_CSV)\n",
    "        time.sleep(2)  # avoid hammering the API\n",
    "\n",
    "# =============================\n",
    "# 9. Main Entry\n",
    "# =============================\n",
    "\n",
    "subreddit_name = input(\"Enter subreddit name: \")\n",
    "try:\n",
    "    comments = fetch_comments_from_subreddit(subreddit_name)\n",
    "    print(f\"Collected {len(comments)} comments from r/{subreddit_name}\")\n",
    "    process_comments(comments)\n",
    "except Exception as e:\n",
    "    print(\"Error:\", e)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ebd23590-64f3-4f06-a524-66dc0586d72a",
   "metadata": {},
   "source": [
    "def save_comments_to_csv(comments, filename=\"reddit_comments.csv\"):\n",
    "    with open(filename, \"w\", encoding=\"utf-8\", newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([\"comment\"])\n",
    "        for comment in comments:\n",
    "            writer.writerow([comment])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
