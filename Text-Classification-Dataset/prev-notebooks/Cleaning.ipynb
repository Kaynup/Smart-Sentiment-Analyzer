{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "acade035-c530-42e8-8571-6cbe0271fa02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_json(\"combined.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ac52e81-1fcf-498b-9452-955877db2af3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2314712 entries, 0 to 2314711\n",
      "Data columns (total 3 columns):\n",
      " #   Column          Dtype \n",
      "---  ------          ----- \n",
      " 0   body            object\n",
      " 1   subreddit       object\n",
      " 2   subreddit_full  object\n",
      "dtypes: object(3)\n",
      "memory usage: 53.0+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "88fbbe66-12a4-4c21-aa00-ecee5fa32ada",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2314712 entries, 0 to 2314711\n",
      "Data columns (total 2 columns):\n",
      " #   Column     Dtype \n",
      "---  ------     ----- \n",
      " 0   body       object\n",
      " 1   subreddit  object\n",
      "dtypes: object(2)\n",
      "memory usage: 35.3+ MB\n"
     ]
    }
   ],
   "source": [
    "df = df.drop(['subreddit_full'], axis=1)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9399432b-0e49-4927-bb8b-674cb47f2b36",
   "metadata": {},
   "source": [
    "# Data cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "112ad021-5cd0-4c66-bfbd-f9f9517cf9c8",
   "metadata": {},
   "source": [
    "## 1 - Removing duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fd0326f4-7e92-4559-a4b6-04501532395c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body</th>\n",
       "      <th>subreddit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2314712</td>\n",
       "      <td>2314712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>2152515</td>\n",
       "      <td>192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>[removed]</td>\n",
       "      <td>r-WinStupidPrizes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>17732</td>\n",
       "      <td>79296</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             body          subreddit\n",
       "count     2314712            2314712\n",
       "unique    2152515                192\n",
       "top     [removed]  r-WinStupidPrizes\n",
       "freq        17732              79296"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e6af8781-9de5-471c-b457-b77b9451de8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicated rows (including originals): 190579\n",
      "These are the duplicate entries in 'body':\n",
      "       body             subreddit\n",
      "0                    r-Paranormal\n",
      "1              r-ShitAmericansSay\n",
      "2               r-stepdadreflexes\n",
      "3                     r-thebutton\n",
      "4               r-WinStupidPrizes\n",
      "...     ...                   ...\n",
      "190574   🫶🏼  r-contagiouslaughter\n",
      "190575   🫶🏼        r-standupshots\n",
      "190576   🫶🏼           r-dank_meme\n",
      "190577  🫶🫶🫶      r-wholesomememes\n",
      "190578  🫶🫶🫶         r-rarepuppers\n",
      "\n",
      "[190579 rows x 2 columns]\n",
      "\n",
      "Number of unique rows after dropping duplicates: 2152515\n",
      "Data after removing duplicates:\n",
      "                                                      body   subreddit\n",
      "0                                                    Brick  r-3amjokes\n",
      "1        Ban wave #2 is complete, please keep reporting...  r-3amjokes\n",
      "2        I'm not an insomniac, but I am narcoleptic, an...  r-3amjokes\n",
      "3                      Hear you loud and clear, thank you.  r-3amjokes\n",
      "4                         I had no idea this sub had mods.  r-3amjokes\n",
      "...                                                    ...         ...\n",
      "2152510  Do you NEED the other POVs to do the story jus...   r-writing\n",
      "2152511  Is it first person or third person? \\n\\nI thin...   r-writing\n",
      "2152512  Maybe I'm just not ambitious enough, but after...   r-writing\n",
      "2152513  My god I love your idea! I really enjoy books ...   r-writing\n",
      "2152514  Don’t do multi pov if you aren’t prepared to c...   r-writing\n",
      "\n",
      "[2152515 rows x 2 columns]\n",
      "\n",
      "Count of each duplicate 'body' text (those that appear more than once):\n",
      "'[removed]...' appears 17732 times\n",
      "'[deleted]...' appears 17147 times\n",
      "'[ Removed by Reddit ]...' appears 1946 times\n",
      "'u/savevideo...' appears 1510 times\n",
      "'Thank you for Posting in r/MovieReviews Please report any ru...' appears 985 times\n",
      "'\n",
      "**    Please don't: \n",
      "\n",
      "- be a dick to other people\n",
      "\n",
      "- incite...' appears 985 times\n",
      "'**Welcome to the Prompt!** All top-level comments must be a ...' appears 967 times\n",
      "'**Reminders for Commenters:**\n",
      "\n",
      "* All responses must be A) si...' appears 928 times\n",
      "'Thank you for being a part of the r/DankChristianMemes commu...' appears 912 times\n",
      "'Welcome to /r/askphilosophy! **Please read [our updated rule...' appears 888 times\n",
      "'Please remember to follow all of our rules. Use the report f...' appears 860 times\n",
      "'\n",
      "**All posts must directly relate to learning one or more SP...' appears 840 times\n",
      "'Hi! This is our community moderation bot.\n",
      "\n",
      "---\n",
      "\n",
      "If this post...' appears 834 times\n",
      "'Remember to change your flair to reflect the appropriate NSF...' appears 792 times\n",
      "'\n",
      "Hello users, welcome to a sub dedicated to freakouts withou...' appears 791 times\n",
      "'\n",
      "Hey, OP! Please reply to this comment to provide context fo...' appears 767 times\n",
      "'If this is suspiciously specific, **Upvote** this comment!\n",
      "\n",
      "...' appears 763 times\n",
      "'Nice...' appears 681 times\n",
      "'If OP's post is funny or otherwise unfitting, please report ...' appears 679 times\n",
      "'Thanks for confirming that you flaired this correctly!...' appears 653 times\n",
      "'Lol...' appears 629 times\n",
      "'**Please report this post if:**\n",
      "\n",
      "* There is no audible laugh...' appears 624 times\n",
      "'Reminder: this subreddit is meant to be a place free of exce...' appears 608 times\n",
      "'Hey Brdies,\n",
      "Just a reminder that you should definitely repor...' appears 600 times\n",
      "'**Thank you for your submission to r/HobbyDrama !**\n",
      "\n",
      "Our rul...' appears 589 times\n",
      "'Yes...' appears 521 times\n",
      "'I am thy PovertyMerchant.Thou shalt PM thou key.\n",
      "\n",
      "P.S. I'm b...' appears 496 times\n",
      "'No...' appears 409 times\n",
      "'\n",
      "Just a friendly reminder that we have our own discord serve...' appears 403 times\n",
      "'u/savevideobot...' appears 390 times\n",
      "'downvote this comment if the meme sucks. upvote it and I'll ...' appears 374 times\n",
      "'Welcome to r/terriblefacebookmemes! It sucks, but it is ours...' appears 371 times\n",
      "'Check out our [Instagram!](https://www.instagram.com/atetheo...' appears 359 times\n",
      "'Thank you for your submission! Don't forget to comment with ...' appears 358 times\n",
      "'lol...' appears 356 times\n",
      "'Wow...' appears 354 times\n",
      "'Lmao...' appears 336 times\n",
      "'To all those viewing this thread, welcome. Please keep in mi...' appears 325 times\n",
      "'😂...' appears 310 times\n",
      "'Hello, Would YOU let ME join YOUR CIRCLE?\n",
      "\n",
      "*You can let me i...' appears 307 times\n",
      "'\n",
      "---\n",
      "---\n",
      "\n",
      "#&#128999;&#129002; THIS IS NOT A GENERAL DISCUSSI...' appears 298 times\n",
      "'---      \n",
      "\n",
      ">This is a friendly reminder to [read our rules](...' appears 296 times\n",
      "'Good...' appears 288 times\n",
      "'r/lostredditors...' appears 285 times\n",
      "'\n",
      "---\n",
      "\n",
      "#**↪ AgainstHateSubreddits F.A.Q.s / HOWTOs / READMEs ...' appears 283 times\n",
      "'Whoa! You win the meme connoisseur title for having over 2k ...' appears 273 times\n",
      "'🤣...' appears 265 times\n",
      "'Beautiful...' appears 263 times\n",
      "'🤣🤣🤣...' appears 252 times\n",
      "'FAFO...' appears 239 times\n",
      "'F...' appears 232 times\n",
      "'😂😂😂...' appears 231 times\n",
      "'u/repostsleuthbot...' appears 222 times\n",
      "'Adorable...' appears 220 times\n",
      "'Pressers gather, and now my watch begins. It shall not end u...' appears 214 times\n",
      "'r/kidsarefuckingstupid...' appears 214 times\n",
      "'Congratulations! I wish you well my friend....' appears 210 times\n",
      "'We've updated our rules! [Please review the full documentati...' appears 207 times\n",
      "'All posts must be links to academic articles about linguisti...' appears 202 times\n",
      "'What?...' appears 202 times\n",
      "'Based...' appears 193 times\n",
      "'No....' appears 185 times\n",
      "'....' appears 185 times\n",
      "'Hello, this notice is to inform you that this subreddit offi...' appears 184 times\n",
      "'r/killthecameraman...' appears 178 times\n",
      "'Hey Brdies,\n",
      "Just a reminder that you should definitely repor...' appears 177 times\n",
      "'❤️...' appears 173 times\n",
      "'Yes....' appears 171 times\n",
      "'I don't get it...' appears 167 times\n",
      "'Hell yeah...' appears 160 times\n",
      "'Thanks for providing a source!...' appears 160 times\n",
      "'Hi...' appears 158 times\n",
      "'If you want to talk about social anxiety, r/socialanxiety is...' appears 156 times\n",
      "'Why?...' appears 154 times\n",
      "'Gross...' appears 152 times\n",
      "'!!! Attention, sussy meme enthusiasts!\n",
      "\n",
      "Listen up, cool crew...' appears 147 times\n",
      "'What...' appears 147 times\n",
      "'Good....' appears 147 times\n",
      "'Who?...' appears 146 times\n",
      "'Wtf...' appears 146 times\n",
      "'Ouch...' appears 145 times\n",
      "'As an appreciation for your content contributions to this co...' appears 145 times\n",
      "'Sending you love and light x...' appears 145 times\n",
      "'![gif](giphy|cO39srN2EUIRaVqaVq)...' appears 141 times\n",
      "'Damn...' appears 140 times\n",
      "'LOL...' appears 140 times\n",
      "'Nope...' appears 139 times\n",
      "'r/donthelpjustfilm...' appears 138 times\n",
      "'Hello and welcome to /r/CircleofTrust !\n",
      "If you are confused ...' appears 134 times\n",
      "'Penis...' appears 134 times\n",
      "'Deserved...' appears 133 times\n",
      "'Love it...' appears 126 times\n",
      "'Bruh...' appears 126 times\n",
      "'Yikes...' appears 125 times\n",
      "'I would love to join your circle! I am friend of many circle...' appears 125 times\n",
      "'Fake...' appears 124 times\n",
      "'Ok...' appears 122 times\n",
      "'Here's a [pupper](https://imgur.com/Cuk1csX) for your time a...' appears 120 times\n",
      "'Haha...' appears 118 times\n",
      "'u/SaveVideo...' appears 118 times\n",
      "'Oof...' appears 117 times\n",
      "'In an effort to improve submission quality, we are now manua...' appears 117 times\n",
      "'Repost...' appears 116 times\n",
      "'r/thathappened...' appears 115 times\n",
      "'I don’t get it...' appears 115 times\n",
      "'🤮...' appears 112 times\n",
      "'Same...' appears 112 times\n",
      "'![gif](giphy|7k2LoEykY5i1hfeWQB)...' appears 111 times\n",
      "'Dumbass...' appears 111 times\n",
      "'👍...' appears 111 times\n",
      "':(...' appears 111 times\n",
      "'r/memesopdidnotlike...' appears 111 times\n",
      "'nice...' appears 110 times\n",
      "'![gif](giphy|J8FZIm9VoBU6Q)...' appears 110 times\n",
      "'u/vredditdownloader...' appears 110 times\n",
      "'Amazing...' appears 109 times\n",
      "'Ew...' appears 108 times\n",
      "'What the fuck...' appears 108 times\n",
      "'?...' appears 107 times\n",
      "'r/watchpeopledieinside...' appears 106 times\n",
      "'Idiot...' appears 106 times\n",
      "'😂😂...' appears 104 times\n",
      "'You look very friendly! God loves you....' appears 104 times\n",
      "'😂😂😂😂...' appears 103 times\n",
      "'If you're wondering what all of this is, [here is an FAQ.](h...' appears 102 times\n",
      "'R.I.P🙏🕊...' appears 99 times\n"
     ]
    }
   ],
   "source": [
    "duplicates_df = df[df.duplicated(subset='body', keep=False)].sort_values(by='body').reset_index(drop=True)\n",
    "print(f\"Number of duplicated rows (including originals): {len(duplicates_df)}\")\n",
    "print(\"These are the duplicate entries in 'body':\")\n",
    "print(duplicates_df)\n",
    "\n",
    "# Step 2: Drop duplicates, keeping only the first occurrence\n",
    "df_1 = df.drop_duplicates(subset='body', keep='first').reset_index(drop=True)\n",
    "print(f\"\\nNumber of unique rows after dropping duplicates: {len(df_1)}\")\n",
    "print(\"Data after removing duplicates:\")\n",
    "print(df_1)\n",
    "\n",
    "# Step 3: Show counts of how many times each duplicate occurs\n",
    "duplicate_counts = df['body'].value_counts()\n",
    "duplicate_counts = duplicate_counts[duplicate_counts > 1]\n",
    "\n",
    "print(\"\\nCount of each duplicate 'body' text (those that appear more than once):\")\n",
    "for text, count in duplicate_counts.items():\n",
    "    print(f\"'{text[:60]}...' appears {count} times\")  # Show first 60 chars to keep output readable\n",
    "    if count < 100:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "252a53d3-b1cd-43f1-8d8d-45b58ae4fd04",
   "metadata": {},
   "source": [
    "## 2 - Removing links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "74492166-753a-4bb4-90bc-00f7775720ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from html import unescape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fd05466a-dc1a-452f-93c8-cc43c777a563",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample cleaned output:\n",
      "\n",
      "                                                body  \\\n",
      "0                                              Brick   \n",
      "1  Ban wave #2 is complete, please keep reporting...   \n",
      "2  I'm not an insomniac, but I am narcoleptic, an...   \n",
      "3                Hear you loud and clear, thank you.   \n",
      "4                   I had no idea this sub had mods.   \n",
      "5  can I be in the subreddit at any other time th...   \n",
      "6    Have a random question: who is on the sub icon?   \n",
      "7                               r/notinteresting lol   \n",
      "8                                        😍😍😍🥰🥰🥰🥰🥰🥰😗😗   \n",
      "9       B döndüm döne dk d dans d dlcn dllflgmglnfmf   \n",
      "\n",
      "                                        body_cleaned  \n",
      "0                                              Brick  \n",
      "1  Ban wave #2 is complete, please keep reporting...  \n",
      "2  I'm not an insomniac, but I am narcoleptic, an...  \n",
      "3                Hear you loud and clear, thank you.  \n",
      "4                   I had no idea this sub had mods.  \n",
      "5  can I be in the subreddit at any other time th...  \n",
      "6    Have a random question: who is on the sub icon?  \n",
      "7                               r/notinteresting lol  \n",
      "8                                        😍😍😍🥰🥰🥰🥰🥰🥰😗😗  \n",
      "9       B döndüm döne dk d dans d dlcn dllflgmglnfmf  \n"
     ]
    }
   ],
   "source": [
    "def strip_links_html(text, verbose=False):\n",
    "    if not isinstance(text, str):\n",
    "        if verbose:\n",
    "            print(f\"Skipping non-string input: {text}\")\n",
    "        return \"\"\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Original:\", text)\n",
    "\n",
    "    # Unescape HTML entities (like &nbsp;)\n",
    "    text = unescape(text)\n",
    "    if verbose:\n",
    "        print(\"After unescape:\", text)\n",
    "\n",
    "    # Remove HTML tags\n",
    "    text = re.sub(r'<[^>]+>', ' ', text)\n",
    "    if verbose:\n",
    "        print(\"After removing HTML tags:\", text)\n",
    "\n",
    "    # Remove URLs\n",
    "    text = re.sub(r'https?://\\S+|www\\.\\S+', ' ', text)\n",
    "    if verbose:\n",
    "        print(\"After removing URLs:\", text)\n",
    "\n",
    "    # Remove HTML-like codes (&gt;, &amp;)\n",
    "    text = re.sub(r'&\\w+;', ' ', text)\n",
    "    if verbose:\n",
    "        print(\"After removing entities:\", text)\n",
    "\n",
    "    # Normalize spaces\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    if verbose:\n",
    "        print(\"Final cleaned text:\", text)\n",
    "        print(\"=\"*50)\n",
    "\n",
    "    return text\n",
    "\n",
    "# Ensure 'df_1' and 'body' column exist\n",
    "if 'df_1' in locals() and 'body' in df_1.columns:\n",
    "    df_2 = df_1.copy()\n",
    "    df_2['body_cleaned'] = df_2['body'].apply(lambda x: strip_links_html(x, verbose=False))\n",
    "    \n",
    "    # ✅ Show sample output\n",
    "    print(\"Sample cleaned output:\\n\")\n",
    "    print(df_2[['body', 'body_cleaned']].head(10))\n",
    "else:\n",
    "    print(\"DataFrame 'df_1' or column 'body' not found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd58d466-da60-489b-a8bb-11bdc5cf20d3",
   "metadata": {},
   "source": [
    "## 3 - Removing non-useful"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8a0831a5-e445-4351-8283-b48593731e2d",
   "metadata": {},
   "source": [
    "Removing samples like 'r/..' or non-english lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d5886dfe-e771-4c4c-bcfe-2538c3baaf97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔎 Searching for 'r/...' patterns in all columns...\n",
      "Found 34070 rows containing 'r/...' patterns.\n",
      "Rows removed: 34070\n",
      "Rows remaining after cleaning: 2118445\n",
      "\n",
      "🔍 Example of removed rows (contain 'r/...'):\n",
      "                           body_cleaned\n",
      "7                  r/notinteresting lol\n",
      "18  Is this r/antijokes but with jokes?\n",
      "\n",
      "Example of cleaned rows (without 'r/...'):\n",
      "                                        body_cleaned\n",
      "0                                              Brick\n",
      "1  Ban wave #2 is complete, please keep reporting...\n"
     ]
    }
   ],
   "source": [
    "# Create a mask to find 'r/...' patterns across all columns\n",
    "print(\"🔎 Searching for 'r/...' patterns in all columns...\")\n",
    "mask_r_style = df_2.apply(lambda x: x.astype(str).str.contains(r'\\br\\/\\w+', regex=True)).any(axis=1)\n",
    "print(f\"Found {mask_r_style.sum()} rows containing 'r/...' patterns.\")\n",
    "\n",
    "# Step 2: Remove those rows and store them separately\n",
    "removed_df = df_2[mask_r_style].copy()\n",
    "df_3 = df_2[~mask_r_style].copy()\n",
    "print(f\"Rows removed: {len(removed_df)}\")\n",
    "print(f\"Rows remaining after cleaning: {len(df_3)}\")\n",
    "\n",
    "# Step 3: Show sample rows\n",
    "print(\"\\n🔍 Example of removed rows (contain 'r/...'):\")\n",
    "print(removed_df[['body_cleaned']].head(2))\n",
    "\n",
    "print(\"\\nExample of cleaned rows (without 'r/...'):\")\n",
    "print(df_3[['body_cleaned']].head(2))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "43e5f62c-03e8-44ff-ab5f-ba5c7f0976f8",
   "metadata": {},
   "source": [
    "We will allow special characters like:-\n",
    "- Punctuations:   .  ,  \"\"  ''  '  !  ?\n",
    "- Others:   :  -\n",
    "- Various Emojies"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5c254aef-d0b2-4f1c-88e0-6c01a4d281ee",
   "metadata": {},
   "source": [
    "Checking for various errors and sentences that are not valid"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6c53eebd-8ad9-47e5-a938-005e97a2bd0b",
   "metadata": {},
   "source": [
    "We can will do that with a powerful model API and directly add meta data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f03cc3b1-22d8-4eff-8a10-fcc7589f54fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 2118445 entries, 0 to 2152514\n",
      "Data columns (total 3 columns):\n",
      " #   Column        Dtype \n",
      "---  ------        ----- \n",
      " 0   body          object\n",
      " 1   subreddit     object\n",
      " 2   body_cleaned  object\n",
      "dtypes: object(3)\n",
      "memory usage: 64.6+ MB\n"
     ]
    }
   ],
   "source": [
    "df_3.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "95510df8-59ce-4301-9b6b-1c80c91214e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2118445 entries, 0 to 2118444\n",
      "Data columns (total 3 columns):\n",
      " #   Column        Dtype \n",
      "---  ------        ----- \n",
      " 0   body          object\n",
      " 1   subreddit     object\n",
      " 2   body_cleaned  object\n",
      "dtypes: object(3)\n",
      "memory usage: 48.5+ MB\n"
     ]
    }
   ],
   "source": [
    "# Index reset\n",
    "data = df_3.copy()\n",
    "data = df_3.reset_index(drop=True)\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "913088a4-4e84-4c12-930b-443d9f5a888c",
   "metadata": {},
   "source": [
    "# Save data in chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cef58c8-99dd-436e-8edf-d484edccd491",
   "metadata": {},
   "source": [
    "### Save csv with 'tilda' as separator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6f1418dc-7579-486b-8229-9aa76b1d701c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: precleaned_chunk_1.csv (400000 rows)\n",
      "\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 400000 entries, 0 to 399999\n",
      "Data columns (total 3 columns):\n",
      " #   Column        Non-Null Count   Dtype \n",
      "---  ------        --------------   ----- \n",
      " 0   body          400000 non-null  object\n",
      " 1   subreddit     400000 non-null  object\n",
      " 2   body_cleaned  400000 non-null  object\n",
      "dtypes: object(3)\n",
      "memory usage: 9.2+ MB\n",
      "\n",
      "\n",
      "\n",
      "Saved: precleaned_chunk_2.csv (400000 rows)\n",
      "\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 400000 entries, 400000 to 799999\n",
      "Data columns (total 3 columns):\n",
      " #   Column        Non-Null Count   Dtype \n",
      "---  ------        --------------   ----- \n",
      " 0   body          400000 non-null  object\n",
      " 1   subreddit     400000 non-null  object\n",
      " 2   body_cleaned  400000 non-null  object\n",
      "dtypes: object(3)\n",
      "memory usage: 9.2+ MB\n",
      "\n",
      "\n",
      "\n",
      "Saved: precleaned_chunk_3.csv (400000 rows)\n",
      "\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 400000 entries, 800000 to 1199999\n",
      "Data columns (total 3 columns):\n",
      " #   Column        Non-Null Count   Dtype \n",
      "---  ------        --------------   ----- \n",
      " 0   body          400000 non-null  object\n",
      " 1   subreddit     400000 non-null  object\n",
      " 2   body_cleaned  400000 non-null  object\n",
      "dtypes: object(3)\n",
      "memory usage: 9.2+ MB\n",
      "\n",
      "\n",
      "\n",
      "Saved: precleaned_chunk_4.csv (400000 rows)\n",
      "\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 400000 entries, 1200000 to 1599999\n",
      "Data columns (total 3 columns):\n",
      " #   Column        Non-Null Count   Dtype \n",
      "---  ------        --------------   ----- \n",
      " 0   body          400000 non-null  object\n",
      " 1   subreddit     400000 non-null  object\n",
      " 2   body_cleaned  400000 non-null  object\n",
      "dtypes: object(3)\n",
      "memory usage: 9.2+ MB\n",
      "\n",
      "\n",
      "\n",
      "Saved: precleaned_chunk_5.csv (518445 rows)\n",
      "\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 518445 entries, 1600000 to 2118444\n",
      "Data columns (total 3 columns):\n",
      " #   Column        Non-Null Count   Dtype \n",
      "---  ------        --------------   ----- \n",
      " 0   body          518445 non-null  object\n",
      " 1   subreddit     518445 non-null  object\n",
      " 2   body_cleaned  518445 non-null  object\n",
      "dtypes: object(3)\n",
      "memory usage: 11.9+ MB\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def save_df_as_csv(df, filename):\n",
    "    with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "        # Write header row\n",
    "        f.write(\"~\".join(f'\"{col}\"' for col in df.columns) + \"\\n\")\n",
    "        \n",
    "        # Write each data row\n",
    "        for _, row in df.iterrows():\n",
    "            quoted = []\n",
    "            for item in row:\n",
    "                # Convert to string and clean\n",
    "                val = str(item).replace('\\n', '.').replace('\\r', '.').replace('\"', '\"\"')\n",
    "                quoted.append(f'\"{val}\"')  # Wrap in double quotes\n",
    "            f.write(\"~\".join(quoted) + \"\\n\")\n",
    "\n",
    "# Split and save in 5 chunks\n",
    "chunk_size = 400000\n",
    "total_rows = len(data)\n",
    "\n",
    "for i in range(5):\n",
    "    start = i * chunk_size\n",
    "    end = start + chunk_size if i < 4 else total_rows  # last chunk takes the rest\n",
    "    chunk_df = data.iloc[start:end]\n",
    "    filename = f\"precleaned_chunk_{i+1}.csv\"\n",
    "    save_df_as_csv(chunk_df, filename)\n",
    "    print(f\"Saved: {filename} ({len(chunk_df)} rows)\\n\\n\")\n",
    "    chunk_df.info()\n",
    "    print(\"\\n\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
